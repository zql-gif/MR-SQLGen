# 参考A Survey on Large Language Models for Code Generation ---- 5.10.3
[A Survey on Large Language Models for Code Generation](Research/A%20Survey%20on%20Large%20Language%20Models%20for%20Code%20Generation)

下面概括LLM as A Judge 的方法AlpacaEval [148] 和 MT-bench [320]和ICE-Score 评估指标[332]
# AlpacaEval [148]
## links
* github：[tatsu-lab/alpaca_eval: An automatic evaluator for instruction-following language models. Human-validated, high-quality, cheap, and fast.](https://github.com/tatsu-lab/alpaca_eval)

# MT-bench this [320]
## links
* 工具论文：[Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://papers.nips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html)
* [MT Bench - a Hugging Face Space by lmsys](https://huggingface.co/spaces/lmsys/mt-bench)

# ICE-Score 评估指标[332]
## links
* [ICE-Score: Instructing Large Language Models to Evaluate Code - ACL Anthology](https://aclanthology.org/2024.findings-eacl.148/)

# conferences
[148] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. AlpacaEval: An Automatic Evaluator of Instruction-following Models. https://github.com/tatsu-lab/alpaca_eval.
[320] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems 36 (2024).
[332] Terry Yue Zhuo. 2024. ICE-Score: Instructing Large Language Models to Evaluate Code. In Findings of the Association for Computational Linguistics: EACL 2024. 2232–2242.






